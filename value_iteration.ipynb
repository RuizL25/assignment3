{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteración de Valores\n",
    "\n",
    "En este ejercicio vamos a implementar el primer método para solucionar Procesos de Decisión de Markov (MDPs). El método a implementar es la iteración de valores.\n",
    "\n",
    "La iteración de valores esta basada en la fórmula:\n",
    "\n",
    "![value_iteration](./img/value_iteration.png)\n",
    "\n",
    "Para resolver los MDPs utilizaremos `value_iteration.py` para la lógica de la iteración de valores y `gridworld_mdp.py` para definir los ambientes de Gridworld y Bridge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T03:56:55.598187Z",
     "iopub.status.busy": "2026-02-24T03:56:55.598110Z",
     "iopub.status.idle": "2026-02-24T03:56:55.687168Z",
     "shell.execute_reply": "2026-02-24T03:56:55.686733Z"
    }
   },
   "outputs": [],
   "source": [
    "from value_iteration import ValueIteration\n",
    "from gridworld_mdp import GridworldMDP, BridgeMDP\n",
    "import numpy as np\n",
    "\n",
    "def print_grid_values(mdp, values, title=\"Valores de Estados\"):\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    grid_display = []\n",
    "    for r in range(mdp.nrows):\n",
    "        row_display = []\n",
    "        for c in range(mdp.ncols):\n",
    "            state = (r, c)\n",
    "            if mdp.board[r][c] == '#':\n",
    "                row_display.append(\"  #  \")\n",
    "            elif mdp.is_terminal(state):\n",
    "                row_display.append(f\"{float(mdp.board[r][c]):+.2f}\")\n",
    "            else:\n",
    "                row_display.append(f\"{values.get(state, 0.0):+.2f}\")\n",
    "        grid_display.append(\" \".join(row_display))\n",
    "    for row in grid_display:\n",
    "        print(row)\n",
    "\n",
    "def print_grid_policy(mdp, policy, title=\"Política de Acciones\"):\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    action_map = {'up': '^^', 'down': 'VV', 'left': '<<', 'right': '>>', 'exit': 'EX', None: '--'}\n",
    "    grid_display = []\n",
    "    for r in range(mdp.nrows):\n",
    "        row_display = []\n",
    "        for c in range(mdp.ncols):\n",
    "            state = (r, c)\n",
    "            if mdp.board[r][c] == '#':\n",
    "                row_display.append(\" # \")\n",
    "            elif mdp.is_terminal(state):\n",
    "                row_display.append(f\" {mdp.board[r][c]:2s} \") # Display reward for terminal states\n",
    "            else:\n",
    "                action = policy.get(state, None)\n",
    "                row_display.append(action_map.get(action, '?') + \" \")\n",
    "        grid_display.append(\" \".join(row_display))\n",
    "    for row in grid_display:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld: Configuración y Experimentos\n",
    "\n",
    "Utilizaremos un tablero de 10x10 para el ambiente de Gridworld, con el modelo de transición de probabilidad uniforme (0.25 para cada acción) como se planteó en la Task 2 de `Assignment_gridworld.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T03:56:55.688356Z",
     "iopub.status.busy": "2026-02-24T03:56:55.688255Z",
     "iopub.status.idle": "2026-02-24T03:56:55.759649Z",
     "shell.execute_reply": "2026-02-24T03:56:55.759284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Resumen de Convergencia =====\n",
      "Iteraciones     | Max Delta (Δ)   | ¿Cambió la Política?\n",
      "-------------------------------------------------------\n",
      "5               | 0.4043          | Base (Inicial)      \n",
      "10              | 0.0708          | No                  \n",
      "15              | 0.0264          | No                  \n",
      "20              | 0.0134          | No                  \n",
      "30              | 0.0101          | No                  \n",
      "50              | 0.0031          | No                  \n",
      "-------------------------------------------------------\n",
      "Nota: El 'Max Delta (Δ)' muestra cuánto cambiaron los valores respecto a la iteración probada anterior.\n"
     ]
    }
   ],
   "source": [
    "board_10x10 = [[' ' for _ in range(10)] for _ in range(10)]\n",
    "board_10x10[0][0] = 'S'\n",
    "for c in [1, 2, 3, 4, 6, 7, 8]: board_10x10[2][c] = '#'\n",
    "for r in [3, 4, 5, 6, 7]: board_10x10[r][4] = '#'\n",
    "board_10x10[4][5] = '-1' # R: -1\n",
    "board_10x10[5][5] = '+1' # R: 1\n",
    "board_10x10[7][5] = '-1' # R: -1\n",
    "board_10x10[7][6] = '-1' # R: -1\n",
    "\n",
    "gridworld_mdp = GridworldMDP(board_10x10, transition_model=\"uniform_0.25\")\n",
    "\n",
    "# PEGA ESTO EN SU LUGAR:\n",
    "iterations_to_test = [5, 10, 15, 20, 30, 50]\n",
    "\n",
    "# Variables para guardar el estado de la iteración anterior\n",
    "previous_values = {}\n",
    "previous_policy = {}\n",
    "\n",
    "print(\"\\n===== Resumen de Convergencia =====\")\n",
    "print(f\"{'Iteraciones':<15} | {'Max Delta (Δ)':<15} | {'¿Cambió la Política?':<20}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for num_iterations in iterations_to_test:\n",
    "    vi_agent = ValueIteration(gridworld_mdp, discount=0.9, iterations=num_iterations)\n",
    "    vi_agent.run_value_iteration()\n",
    "    \n",
    "    current_values = vi_agent.values\n",
    "    current_policy = {state: vi_agent.get_policy(state) for state in gridworld_mdp.get_states()}\n",
    "    \n",
    "    if previous_values:\n",
    "        max_delta = max(abs(current_values.get(s, 0.0) - previous_values.get(s, 0.0)) for s in gridworld_mdp.get_states())\n",
    "    else:\n",
    "        max_delta = max(abs(current_values.get(s, 0.0)) for s in gridworld_mdp.get_states()) \n",
    "        \n",
    "    if previous_policy:\n",
    "        policy_changed = current_policy != previous_policy\n",
    "        cambio_str = \"Sí\" if policy_changed else \"No\"\n",
    "    else:\n",
    "        cambio_str = \"Base (Inicial)\"\n",
    "        \n",
    "    print(f\"{num_iterations:<15} | {max_delta:<15.4f} | {cambio_str:<20}\")\n",
    "    \n",
    "    previous_values = current_values.copy()\n",
    "    previous_policy = current_policy.copy()\n",
    "\n",
    "print(\"-\" * 55)\n",
    "print(\"Nota: El 'Max Delta (Δ)' muestra cuánto cambiaron los valores respecto a la iteración probada anterior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de los resultados, observamos una propiedad empírica clave del algoritmo de Iteración de Valores: la política óptima converge mucho más rápido que los valores de los estados. Mientras que la función de valor continúa refinándose numéricamente hacia un estado estacionario —evidenciado por la reducción constante del cambio máximo (Max Delta) de 0.4043 en las primeras 5 iteraciones a apenas 0.0031 en la iteración 50—, las acciones óptimas a tomar no sufren ninguna alteración desde la iteración 5. Esta estabilización temprana demuestra que el algoritmo encuentra el orden relativo correcto de las acciones mucho antes de calcular el valor numérico exacto de cada estado, garantizando que el agente esté listo para tomar decisiones óptimas en el MDP desde las etapas iniciales del proceso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bridge Environment: Configuración y Experimentos\n",
    "\n",
    "El ambiente del puente se define como una matriz de `3x7`:\n",
    "\n",
    "- Filas 0 y 2: Tienen recompensa -100 entre las columnas 2 y 5 (inclusive). Las demás celdas son estados normales (recompensa 0).\n",
    "- Fila 1: Es el puente. La entrada es en `(1,0)` (estado inicial 'S') y la salida en `(1,6)` con recompensa +100.\n",
    "\n",
    "Analizaremos el comportamiento con factores de descuento de 0.9 y 0.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T03:56:55.776937Z",
     "iopub.status.busy": "2026-02-24T03:56:55.776822Z",
     "iopub.status.idle": "2026-02-24T03:56:55.780371Z",
     "shell.execute_reply": "2026-02-24T03:56:55.779963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Bridge Environment (Discount = 0.9) =====\n",
      "\n",
      "--- Valores de Estados (Discount=0.9) ---\n",
      "+53.14 +59.05 -100.00 -100.00 -100.00 -100.00 +100.00\n",
      "+59.05 +65.61 +72.90 +81.00 +90.00 +100.00 +100.00\n",
      "+53.14 +59.05 -100.00 -100.00 -100.00 -100.00 +100.00\n",
      "\n",
      "--- Política Optima (Discount=0.9) ---\n",
      "VV  VV   -100   -100   -100   -100  VV \n",
      ">>  >>  >>  >>  >>  >>   +100 \n",
      "^^  ^^   -100   -100   -100   -100  ^^ \n",
      "\n",
      "===== Bridge Environment (Discount = 0.1) =====\n",
      "\n",
      "--- Valores de Estados (Discount=0.1) ---\n",
      "+0.00 +0.00 -100.00 -100.00 -100.00 -100.00 +100.00\n",
      "+0.00 +0.01 +0.10 +1.00 +10.00 +100.00 +100.00\n",
      "+0.00 +0.00 -100.00 -100.00 -100.00 -100.00 +100.00\n",
      "\n",
      "--- Política Optima (Discount=0.1) ---\n",
      "VV  VV   -100   -100   -100   -100  VV \n",
      ">>  >>  >>  >>  >>  >>   +100 \n",
      "^^  ^^   -100   -100   -100   -100  ^^ \n"
     ]
    }
   ],
   "source": [
    "bridge_board = [\n",
    "    [' ', ' ', '-100', '-100', '-100', '-100', ' '],\n",
    "    ['S', ' ', ' ',    ' ',    ' ',    ' ',    '+100'],\n",
    "    [' ', ' ', '-100', '-100', '-100', '-100', ' ']\n",
    "]\n",
    "\n",
    "bridge_mdp = BridgeMDP(bridge_board, transition_model=\"deterministic\")\n",
    "\n",
    "print(\"\\n===== Bridge Environment (Discount = 0.9) =====\")\n",
    "vi_agent_09 = ValueIteration(bridge_mdp, discount=0.9, iterations=10)\n",
    "vi_agent_09.run_value_iteration()\n",
    "policy_09 = {state: vi_agent_09.get_policy(state) for state in bridge_mdp.get_states()}\n",
    "print_grid_values(bridge_mdp, vi_agent_09.values, \"Valores de Estados (Discount=0.9)\")\n",
    "print_grid_policy(bridge_mdp, policy_09, \"Política Optima (Discount=0.9)\")\n",
    "\n",
    "print(\"\\n===== Bridge Environment (Discount = 0.1) =====\")\n",
    "vi_agent_01 = ValueIteration(bridge_mdp, discount=0.1, iterations=10)\n",
    "vi_agent_01.run_value_iteration()\n",
    "policy_01 = {state: vi_agent_01.get_policy(state) for state in bridge_mdp.get_states()}\n",
    "print_grid_values(bridge_mdp, vi_agent_01.values, \"Valores de Estados (Discount=0.1)\")\n",
    "print_grid_policy(bridge_mdp, policy_01, \"Política Optima (Discount=0.1)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis del Factor de Descuento en Bridge\n",
    "\n",
    "- Con un **descuento alto (e.g., 0.9)**, el agente considera las recompensas futuras casi tan valiosas como las inmediatas. Esto lo incentivará a buscar la recompensa de +100 en el puente, incluso si eso significa pasar por más pasos para llegar allí, evitando las recompensas negativas de -100.\n",
    "\n",
    "- Con un **descuento bajo (e.g., 0.1)**, el agente prioriza las recompensas inmediatas. Las recompensas futuras se devalúan rápidamente. En este caso, el agente podría ser más propenso a tomar caminos más cortos, incluso si esto conlleva el riesgo de encontrarse con una recompensa negativa en el futuro, ya que el impacto de esa recompensa negativa futura es mucho menor. Podría optar por evitar el puente si un camino más corto lo lleva a una recompensa (o penalización) que no está demasiado lejos, o podría simplemente quedarse quieto si no ve una recompensa significativa a corto plazo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f5892956ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

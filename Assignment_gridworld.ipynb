{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c9a151",
   "metadata": {},
   "source": [
    "### Luis Fernando Jose Ruiz Ortega\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a0a6b89-5f5b-4706-8da8-c8d00ff69870",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gridworld y su soluci贸n como MDPs\n",
    "\n",
    "En este trabajo definiremos el ambiente de Gridworld y su soluci贸n como un MDP.\n",
    "Gridworld es un ambiente cl谩sico de prueba dentro del aprendizaje por refuerzo. Durante este taller definiremos el modelo b谩sico del ambiente, que extenderemos incrementalmente de acuerdo a las necesidades del algoritmo de soluci贸n.\n",
    "\n",
    "## Ambiente \n",
    "\n",
    "El ambiente de gridworld se define como una cuadricula de `nxm`. El ambiente tiene obstaculos, es decir casillas por las cuales no puede pasar el agente. Al chocar con un obstaculo, el agente terminar铆a en el mismo estado inicial. Adem谩s, el ambiente tiene una casilla de inicio, y algunas casillas de salida. Un ejemplo del ambiente para el caso `3x4` se muestra a continuaci贸n.\n",
    "\n",
    "![gridworld.png](./img/gridworld.png)\n",
    "\n",
    "En este ejemplo del ambiente el agente comienza en la casilla inferior izquierda y tiene como objetivo llegar a la casilla de salida verde, con recompensa 1. La otra casilla de salida, tiene recompensa -1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d6e0f1",
   "metadata": {},
   "source": [
    "### Task 1.\n",
    "\n",
    "#### 驴C贸mo podemos codificar el ambiente?\n",
    "\n",
    "De una definici贸n completa del ambiente, como una clase de python llamada `Environment`, estableciendo:\n",
    "\n",
    "1. Un atributo que define la cuadr铆cula (`board`). El ambiente recibir谩 una matriz como par谩metro describiendo la cuadr铆cula en el momento de su creaci贸n. Definiremos las casillas por las que puede pasar el agente como casillas vacias, las casillas por las que no puede pasar el agente con un valor none `#` y las casillas de salida con el valor asociado a la recompensa definidas para cada una de ellas.\n",
    "2. Un atributo `nrows` para almacenar la cantidad de filas de la cuadr铆cula.\n",
    "3. Un atributo `ncols` para almacenar la cantidad de columnas de la cuadr铆cula.\n",
    "4. Un atributo `initial_state` para almacenar el estado inicial del agente dentro del ambiente.\n",
    "5. Un atributo con el estado actual (`current_state`) en el que se encuentra el agente. El valor de `current_state` se definir谩 como una tupla\n",
    "6. Un atributo `P` que guarda la matriz de probabilidades de cada una de las acciones para cada estado. Dicha matriz esta definida por par谩metro.\n",
    "\n",
    "Un ejemplo de la definici贸n del tablero para el caso de 5x5 de la figura anterior se da a continuaci贸n.\n",
    "\n",
    "```\n",
    "board = [['', ' ', ' ',  '+1'],\n",
    "         [' ', '#', ' ',  '-1'],\n",
    "         ['S', ' ', ' ', ' ']]\n",
    "```\n",
    "\n",
    "En el ejemplo `S` denota el estado inicial y `'#'` la casilla prohibida (manejaremos esta convenci贸n para todos los ambientes de gridworld).\n",
    "\n",
    "De forma similar a la definici贸n del `board` la matriz de probabilidades `P` se define como:\n",
    "\n",
    "```\n",
    "P = [[[0.1, 0.1, 0, 0.8], [0.1, 0.1, 0, 0.8], [0.1, 0.1, 0, 0.8],  [1]],\n",
    "         [[0.8, 0, 0.1, 0.1], '#', [0.8, 0, 0.1, 0.1],  [-1]],\n",
    "         [[0.8, 0, 0.1, 0.1], [0.1, 0.1, 0.8, 0], [0.1, 0.1, 0.8, 0], [0.1, 0.1, 0.8, 0]]]\n",
    "```\n",
    "\n",
    "Para la definici贸n de `P` vamos a entender cada una de las posiciones de la probabilidad en el orden (`'up', 'down', 'left', 'right'`). Adicionalmente, vamos a suponer que la casilla da la probabilidad de tal forma que el agente siempre toma la acci贸n en direcci贸n al objetivo (la acci贸n que tiene probabilidad `0.8`). Por ejemplo, para la casilla superior izquierda la probabilidad de tomar la acci贸n `right` y llegar a la casilla de arriba es de `0.1`, a la casilla de abajo con probabilidad `0.1` y a la casilla de la derecha con probabilidad `0.8` (el agente nunca puede llegar a la casilla de la izquierda). En las casillas de salida, el agente solo tiene una posibilidad que es tomar la acci贸n `exit` que le da la recompensa asociada a la casilla al agente.\n",
    "\n",
    "#### Comportamiento del ambiente\n",
    "\n",
    "Una vez definido el ambiente definimos su comportamiento. Para ello requerimos los siguientes m茅todos:\n",
    "\n",
    "1. `get_current_state` que no recibe par谩metros y retorna el estado actual (la casilla donde se encuentra el agente)\n",
    "2. `get_posible_actions` que recibe el estado actual del agente como par谩metro y retorna las acciones disponibles para dicho estado. Las acciones estar谩n dadas por su nombre (`'up', 'down', 'left', 'right'`) para las casillas normales y (`'exit'`) para las casillas de salida. Como convenci贸n definiremas que el agente siempre puede moverse en todas las direcciones, donde un movimiento en direcci贸n de un obst谩culo o los l铆mites del ambiente no tienen ning煤n efecto visible en la posici贸n del agente.\n",
    "3. `do_action` que recibe como par谩metro la acci贸n a ejecutar y retorna el valor de la recompensa y el nuevo estado del agente, como un pareja `reward, new_state`. Note que `do_action` esta restringida por la matriz de probabilidad `P` para la ejecuci贸n real de las acciones.\n",
    "4. `reset` que no recibe par谩metros y restablece el ambiente a su estado inicial.\n",
    "5. `is_terminal` que no recibe par谩metros y determina si el agente est谩 en el estado final o no. En nuestro caso, el estado final estar谩 determinado por las casillas de salida (i.e., con un valor definido).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf01f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    board : list[list]\n",
    "    nrows : int\n",
    "    ncols : int\n",
    "    initial_state : tuple\n",
    "    current_state : tuple\n",
    "    P : list[list[list[float]]]\n",
    "    \n",
    "    def __init__(self, board, P):\n",
    "        self.board = board\n",
    "        self.P = P\n",
    "        self.nrows = len(board)\n",
    "        self.ncols = len(board[0])\n",
    "        self.initial_state = (0, 0) \n",
    "        self.current_state = (0, 0)\n",
    "        \n",
    "        for r in range(self.nrows):\n",
    "            for c in range(self.ncols):\n",
    "                if self.board[r][c] == 'S':\n",
    "                    self.initial_state = (r, c)\n",
    "                    self.current_state = (r, c)\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    def get_current_state(self):\n",
    "        return self.current_state\n",
    "    \n",
    "    def get_posible_actions(self, current_state):\n",
    "        r, c = current_state\n",
    "        probs = self.P[r][c]\n",
    "        \n",
    "        if len(probs) == 1 or probs == '#':\n",
    "            return None\n",
    "            \n",
    "        actions_names = ['up', 'down', 'left', 'right']\n",
    "        \n",
    "        for i, prob in enumerate(probs):\n",
    "            if prob == 0.8:\n",
    "                return actions_names[i]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def do_action(self, action):\n",
    "        r, c = self.current_state\n",
    "        \n",
    "        if self.is_terminal(): \n",
    "            return self.board[self.current_state[0]][self.current_state[1]]\n",
    "\n",
    "        dr, dc = 0, 0\n",
    "        if action == 'up':    dr, dc = -1, 0\n",
    "        elif action == 'down':  dr, dc = 1, 0\n",
    "        elif action == 'left':  dr, dc = 0, -1\n",
    "        elif action == 'right': dr, dc = 0, 1\n",
    "        \n",
    "        next_r = r + dr\n",
    "        next_c = c + dc\n",
    "        \n",
    "        \n",
    "        if not (0 <= next_r < self.nrows and 0 <= next_c < self.ncols):\n",
    "            new_state = (r, c)\n",
    "            reward = 0\n",
    "            \n",
    "        elif self.board[next_r][next_c] == '#':\n",
    "            self.reset()\n",
    "            \n",
    "        else:\n",
    "            new_state = (next_r, next_c)\n",
    "            reward = 0\n",
    "            \n",
    "            casilla_destino = self.board[next_r][next_c]\n",
    "            if casilla_destino not in ['', ' ', 'S']:\n",
    "                reward = float(casilla_destino)\n",
    "\n",
    "        self.current_state = new_state\n",
    "        \n",
    "        return reward, new_state\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_state = self.initial_state\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        r, c = self.current_state\n",
    "        casilla = self.board[r][c]\n",
    "        \n",
    "        no_terminales = ['', ' ', 'S', '#']\n",
    "        \n",
    "        if casilla not in no_terminales:\n",
    "            return True \n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "3b9b1f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio en: (2, 0)\n",
      "Paso 0: Estaba en (2, 0) -> Acci贸n: 'up' -> Fui a (1, 0). Reward: 0\n",
      "Paso 1: Estaba en (1, 0) -> Acci贸n: 'up' -> Fui a (0, 0). Reward: 0\n",
      "Paso 2: Estaba en (0, 0) -> Acci贸n: 'right' -> Fui a (0, 1). Reward: 0\n",
      "Paso 3: Estaba en (0, 1) -> Acci贸n: 'right' -> Fui a (0, 2). Reward: 0\n",
      "Paso 4: Estaba en (0, 2) -> Acci贸n: 'right' -> Fui a (0, 3). Reward: 1.0\n",
      "Llegu茅 a un estado final con recompensa: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de tablero\n",
    "\n",
    "board = [['', ' ', ' ',  '+1'],\n",
    "         [' ', '#', ' ',  '-1'],\n",
    "         ['S', ' ', ' ', ' ']]\n",
    "\n",
    "P = [[[0.1, 0.1, 0, 0.8], [0.1, 0.1, 0, 0.8], [0.1, 0.1, 0, 0.8],  [1]],\n",
    "     [[0.8, 0, 0.1, 0.1], '#', [0.8, 0, 0.1, 0.1],  [-1]],\n",
    "     [[0.8, 0, 0.1, 0.1], [0.1, 0.1, 0.8, 0], [0.1, 0.1, 0.8, 0], [0.1, 0.1, 0.8, 0]]]\n",
    "\n",
    "env = Environment(board, P)\n",
    "print(f\"Inicio en: {env.current_state}\")\n",
    "\n",
    "\n",
    "pasos = 0\n",
    "while not env.is_terminal():\n",
    "    \n",
    "    accion_a_tomar = env.get_posible_actions(env.current_state)\n",
    "    \n",
    "    if accion_a_tomar is None:\n",
    "        print(\"Error: No encontr茅 acci贸n de 0.8 o estoy en un lugar invalido\")\n",
    "        break\n",
    "        \n",
    "    estado_anterior = env.current_state\n",
    "    reward, nuevo_estado = env.do_action(accion_a_tomar)\n",
    "    \n",
    "    print(f\"Paso {pasos}: Estaba en {estado_anterior} -> Acci贸n: '{accion_a_tomar}' -> Fui a {nuevo_estado}. Reward: {reward}\")\n",
    "    \n",
    "    pasos += 1\n",
    "\n",
    "if env.is_terminal():\n",
    "    print(f\"Llegu茅 a un estado final con recompensa: {reward}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "598cc01e-d7a7-4cb5-b72b-2d2adf013c9b",
   "metadata": {},
   "source": [
    "Teniendo en cuenta la definici贸n del agente, genere un ambiente de `10x10` como se muestra a continuaci贸n.\n",
    "\n",
    "![evaluacion.png](./img/evaluacion.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3453c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablero 10x10 generado:\n",
      "['S', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "[' ', '#', '#', '#', '#', ' ', '#', '#', '#', ' ']\n",
      "[' ', ' ', ' ', ' ', '#', ' ', ' ', ' ', ' ', ' ']\n",
      "[' ', ' ', ' ', ' ', '#', '-1', ' ', ' ', ' ', ' ']\n",
      "[' ', ' ', ' ', ' ', '#', '+1', ' ', ' ', ' ', ' ']\n",
      "[' ', ' ', ' ', ' ', '#', ' ', ' ', ' ', ' ', ' ']\n",
      "[' ', ' ', ' ', ' ', '#', '-1', '-1', ' ', ' ', ' ']\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "board_10x10 = [[' ' for _ in range(10)] for _ in range(10)]\n",
    "\n",
    "board_10x10[0][0] = 'S'\n",
    "\n",
    "for c in [1, 2, 3, 4, 6, 7, 8]:\n",
    "    board_10x10[2][c] = '#'\n",
    "\n",
    "for r in [3, 4, 5, 6, 7]:\n",
    "    board_10x10[r][4] = '#'\n",
    "\n",
    "board_10x10[4][5] = '-1' # R: -1\n",
    "board_10x10[5][5] = '+1' # R: 1\n",
    "board_10x10[7][5] = '-1' # R: -1\n",
    "board_10x10[7][6] = '-1' # R: -1\n",
    "\n",
    "print(\"Tablero 10x10 generado:\")\n",
    "for row in board_10x10:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c159bab",
   "metadata": {},
   "source": [
    "### Task 2.\n",
    "\n",
    "Plantee el problema de MDP para cada una de las casillas. Especifique el estado de inicio, las transiciones y su probabilidad (suponiendo que todas las acciones sucede con probabilidad de 0.25) y los estados de fin con su recompensa.\n",
    "驴C贸mo ser铆an las recompensas esperadas para cada estado?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eda50b",
   "metadata": {},
   "source": [
    "- Estados(S) = Son las 100 casillas, coordenadas (fila, columna) desde (0,0) hasta (9,9).\n",
    "\n",
    "- Acciones = up, down left, rigth\n",
    "\n",
    "- Probabilidades:\n",
    "  - 0,25 para las 4 direcciones\n",
    "\n",
    "- Recompensas:\n",
    "  - +1 (5,5)\n",
    "  - -1 (4,5), (7,5), (7,6)\n",
    "  - 0 Demas casillas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "8f81137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Matriz de Recompensas Esperadas ---\n",
      "  .     .     .     .     .     .     .     .     .     .   \n",
      "  .     .     .     .     .     .     .     .     .     .   \n",
      "  .     #     #     #     #     .     #     #     #     .   \n",
      "  .     .     .     .     #   -0.25   .     .     .     .   \n",
      "  .     .     .     .     #     .   -0.25   .     .     .   \n",
      "  .     .     .     .     #     .   +0.25   .     .     .   \n",
      "  .     .     .     .     #     .   -0.25   .     .     .   \n",
      "  .     .     .     .     #     .     .   -0.25   .     .   \n",
      "  .     .     .     .     .   -0.25 -0.25   .     .     .   \n",
      "  .     .     .     .     .     .     .     .     .     .   \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcular_recompensas_esperadas(board):\n",
    "    rows = len(board)\n",
    "    cols = len(board[0])\n",
    "    \n",
    "    expected_rewards = np.zeros((rows, cols))\n",
    "    \n",
    "    actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    \n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if board[r][c] in ['#', '+1', '-1']:\n",
    "                continue\n",
    "            \n",
    "            total_reward = 0\n",
    "            \n",
    "            for dr, dc in actions:\n",
    "                nr, nc = r + dr, c + dc\n",
    "                \n",
    "                r_val = 0\n",
    "                \n",
    "                if not (0 <= nr < rows and 0 <= nc < cols):\n",
    "                    r_val = 0\n",
    "                \n",
    "                elif board[nr][nc] == '#':\n",
    "                    r_val = 0 \n",
    "                \n",
    "                elif board[nr][nc] in ['+1', '-1']:\n",
    "                    r_val = float(board[nr][nc])\n",
    "                \n",
    "                else:\n",
    "                    r_val = 0\n",
    "                \n",
    "                total_reward += r_val\n",
    "            \n",
    "            expected_rewards[r][c] = total_reward * 0.25\n",
    "            \n",
    "    return expected_rewards\n",
    "\n",
    "\n",
    "matriz_esperada = calcular_recompensas_esperadas(board_10x10)\n",
    "\n",
    "print(\"--- Matriz de Recompensas Esperadas ---\")\n",
    "for r in range(10):\n",
    "    fila_str = \"\"\n",
    "    for c in range(10):\n",
    "        val = matriz_esperada[r][c]\n",
    "        if board_10x10[r][c] == '#':\n",
    "            fila_str += \"  #   \"\n",
    "        elif val == 0:\n",
    "            fila_str += \"  .   \"\n",
    "        else:\n",
    "            fila_str += f\"{val:+.2f} \"\n",
    "    print(fila_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c2a13",
   "metadata": {},
   "source": [
    "### Task 3.\n",
    "\n",
    "Bajo la definci贸n del problema anterior, suponga que cada acci贸n tiene una probabilidad de 茅xito de 60%, con probabilidad de 20% se ejecutar谩 la sigiente acci贸n (en direcci贸n de las manesillas del reloj), con probabilidad de 10% se ejecutar谩 la sigiente acci贸n (en contra de las manesillas del reloj) y con probabilidad de 10% no pasar谩 nada. Bajo estas condiciones, 驴C贸mo ser铆an las recompensas esperadas para cada estado?\n",
    "\n",
    "Codifique el ambiente para el gridworld de `10x10` utilizando esta funci贸n de probabilidad. En esta codificaci贸n del ambiente no es necesario pasar la matriz `P` como par谩metro, pero esta informaci贸n se debe tener en cuenta en la funci贸n `do_action`.\n",
    "\n",
    "Tenga en cuenta que la calidad del programa que entreguen ser谩 tenida en cuentra dentro de la calificaci贸n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "529c29fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Matriz de Recompensas Esperadas---\n",
      "  .     .     .     .     .     .     .     .     .     .   \n",
      "  .     .     .     .     .     .     .     .     .     .   \n",
      "  .     #     #     #     #     .     #     #     #     .   \n",
      "  .     .     .     .     #   -0.225   .     .     .     .   \n",
      "  .     .     .     .     #     .   -0.225   .     .     .   \n",
      "  .     .     .     .     #     .   +0.225   .     .     .   \n",
      "  .     .     .     .     #   +0.000 -0.225   .     .     .   \n",
      "  .     .     .     .     #     .     .   -0.225   .     .   \n",
      "  .     .     .     .     .   -0.225 -0.225   .     .     .   \n",
      "  .     .     .     .     .     .     .     .     .     .   \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcular_recompensas(board):\n",
    "    rows = len(board)\n",
    "    cols = len(board[0])\n",
    "    \n",
    "    expected_rewards = np.zeros((rows, cols))\n",
    "    \n",
    "    moves = {\n",
    "        'up':    (-1, 0),\n",
    "        'right': (0, 1),\n",
    "        'down':  (1, 0),\n",
    "        'left':  (0, -1)\n",
    "    }\n",
    "    ordered_actions = ['up', 'right', 'down', 'left']\n",
    "    \n",
    "    prob_success = 0.6\n",
    "    prob_cw      = 0.2  \n",
    "    prob_ccw     = 0.1 \n",
    "    prob_stay    = 0.1\n",
    "    \n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if board[r][c] in ['#', '+1', '-1']:\n",
    "                continue\n",
    "            \n",
    "            sum_expected_action_rewards = 0\n",
    "            \n",
    "            for intended_action in ordered_actions:\n",
    "                idx = ordered_actions.index(intended_action)\n",
    "                \n",
    "                outcomes = [\n",
    "                    (ordered_actions[idx], prob_success),          \n",
    "                    (ordered_actions[(idx + 1) % 4], prob_cw),      \n",
    "                    (ordered_actions[(idx - 1) % 4], prob_ccw),     \n",
    "                    ('stay', prob_stay)                            \n",
    "                ]\n",
    "                \n",
    "                action_reward = 0\n",
    "                \n",
    "                for real_move_name, prob in outcomes:\n",
    "                    dr, dc = 0, 0\n",
    "                    if real_move_name != 'stay':\n",
    "                        dr, dc = moves[real_move_name]\n",
    "                    \n",
    "                    nr, nc = r + dr, c + dc\n",
    "                    \n",
    "                    val_reward = 0\n",
    "                    if not (0 <= nr < rows and 0 <= nc < cols):\n",
    "                        val_reward = 0\n",
    "                    elif board[nr][nc] == '#':\n",
    "                        val_reward = 0 \n",
    "                    else:\n",
    "                        contenido = board[nr][nc]\n",
    "                        if contenido in ['+1', '-1']:\n",
    "                            val_reward = float(contenido)\n",
    "                        else:\n",
    "                            val_reward = 0\n",
    "                            \n",
    "                    action_reward += prob * val_reward\n",
    "                \n",
    "                sum_expected_action_rewards += action_reward\n",
    "            \n",
    "            expected_rewards[r][c] = sum_expected_action_rewards * 0.25\n",
    "            \n",
    "    return expected_rewards\n",
    "\n",
    "matriz = calcular_recompensas(board_10x10)\n",
    "\n",
    "print(\"--- Matriz de Recompensas Esperadas---\")\n",
    "for r in range(10):\n",
    "    fila_str = \"\"\n",
    "    for c in range(10):\n",
    "        val = matriz[r][c]\n",
    "        if board_10x10[r][c] == '#':\n",
    "            fila_str += \"  #   \"\n",
    "        elif val == 0:\n",
    "            fila_str += \"  .   \"\n",
    "        else:\n",
    "            fila_str += f\"{val:+.3f} \"\n",
    "    print(fila_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd13d03",
   "metadata": {},
   "source": [
    "## Simulacion task3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "class EnvironmentTask3:\n",
    "    def __init__(self, board):\n",
    "        self.board = board\n",
    "        self.nrows = len(board)\n",
    "        self.ncols = len(board[0])\n",
    "        self.initial_state = (0, 0)\n",
    "        self.current_state = (0, 0)\n",
    "        \n",
    "        for r in range(self.nrows):\n",
    "            for c in range(self.ncols):\n",
    "                if self.board[r][c] == 'S':\n",
    "                    self.initial_state = (r, c)\n",
    "                    self.current_state = (r, c)\n",
    "                    break\n",
    "\n",
    "    def is_terminal(self):\n",
    "        r, c = self.current_state\n",
    "        return self.board[r][c] not in ['', ' ', 'S', '#']\n",
    "\n",
    "    def do_action(self, intended_action):\n",
    "        \"\"\"\n",
    "        L贸gica:\n",
    "        - 60% Intento original\n",
    "        - 20% Clockwise (90掳 derecha)\n",
    "        - 10% Counter-Clockwise (90掳 izquierda)\n",
    "        - 10% Stay (Quieto)\n",
    "        \"\"\"\n",
    "        directions = ['up', 'right', 'down', 'left']\n",
    "        \n",
    "        if intended_action not in directions:\n",
    "            return 0, self.current_state\n",
    "\n",
    "        idx = directions.index(intended_action)\n",
    "        \n",
    "        act_success = directions[idx]             \n",
    "        act_cw      = directions[(idx + 1) % 4]   \n",
    "        act_ccw     = directions[(idx - 1) % 4]   \n",
    "        act_stay    = 'stay'\n",
    "        \n",
    "        outcomes = [act_success, act_cw, act_ccw, act_stay]\n",
    "        probs    = [0.6,         0.2,    0.1,     0.1]\n",
    "        \n",
    "        real_move = random.choices(outcomes, weights=probs, k=1)[0]\n",
    "        \n",
    "        r, c = self.current_state\n",
    "        dr, dc = 0, 0\n",
    "        \n",
    "        if real_move == 'up':    dr, dc = -1, 0\n",
    "        elif real_move == 'down':  dr, dc = 1, 0\n",
    "        elif real_move == 'left':  dr, dc = 0, -1\n",
    "        elif real_move == 'right': dr, dc = 0, 1\n",
    "        \n",
    "        next_r, next_c = r + dr, c + dc\n",
    "        \n",
    "        reward = 0\n",
    "        final_state = (r, c) \n",
    "        \n",
    "        if 0 <= next_r < self.nrows and 0 <= next_c < self.ncols:\n",
    "            contenido = self.board[next_r][next_c]\n",
    "            \n",
    "            if contenido == '#':\n",
    "                final_state = self.initial_state\n",
    "            else:\n",
    "                final_state = (next_r, next_c)\n",
    "                \n",
    "                if contenido not in ['', ' ', 'S']:\n",
    "                    reward = float(contenido)\n",
    "\n",
    "        self.current_state = final_state\n",
    "        return reward, final_state, real_move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "67de3f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio en: (0, 0)\n",
      "\n",
      "Paso 0: Intent贸 'UP' -> Sali贸 'up' Choque\n",
      "        Posici贸n: (0, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 1: Intent贸 'UP' -> Sali贸 'up' Choque\n",
      "        Posici贸n: (0, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 2: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (0, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 3: Intent贸 'DOWN' -> Sali贸 'stay' (Se qued贸 quieto)\n",
      "        Posici贸n: (0, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 4: Intent贸 'DOWN' -> Sali贸 'down' \n",
      "        Posici贸n: (1, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 5: Intent贸 'LEFT' -> Sali贸 'up' Cambio\n",
      "        Posici贸n: (0, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 6: Intent贸 'UP' -> Sali贸 'right' Cambio\n",
      "        Posici贸n: (0, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 7: Intent贸 'UP' -> Sali贸 'up' \n",
      "        Posici贸n: (0, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 8: Intent贸 'DOWN' -> Sali贸 'down' \n",
      "        Posici贸n: (1, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 9: Intent贸 'UP' -> Sali贸 'up' \n",
      "        Posici贸n: (0, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 10: Intent贸 'UP' -> Sali贸 'up' \n",
      "        Posici贸n: (0, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 11: Intent贸 'DOWN' -> Sali贸 'left' Cambio\n",
      "        Posici贸n: (0, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 12: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (0, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 13: Intent贸 'LEFT' -> Sali贸 'up' Cambio\n",
      "        Posici贸n: (0, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 14: Intent贸 'DOWN' -> Sali贸 'down' \n",
      "        Posici贸n: (1, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 15: Intent贸 'LEFT' -> Sali贸 'up' Cambio\n",
      "        Posici贸n: (0, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 16: Intent贸 'DOWN' -> Sali贸 'down' \n",
      "        Posici贸n: (1, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 17: Intent贸 'RIGHT' -> Sali贸 'down' Cambio\n",
      "        Posici贸n: (0, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 18: Intent贸 'UP' -> Sali贸 'up' Choque\n",
      "        Posici贸n: (0, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 19: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (0, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 20: Intent贸 'LEFT' -> Sali贸 'stay' (Se qued贸 quieto)\n",
      "        Posici贸n: (0, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 21: Intent贸 'LEFT' -> Sali贸 'down' Cambio\n",
      "        Posici贸n: (1, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 22: Intent贸 'LEFT' -> Sali贸 'stay' (Se qued贸 quieto)\n",
      "        Posici贸n: (1, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 23: Intent贸 'LEFT' -> Sali贸 'left' \n",
      "        Posici贸n: (1, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 24: Intent贸 'DOWN' -> Sali贸 'left' Cambio\n",
      "        Posici贸n: (1, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 25: Intent贸 'UP' -> Sali贸 'up' Choque\n",
      "        Posici贸n: (0, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 26: Intent贸 'UP' -> Sali贸 'up' Choque\n",
      "        Posici贸n: (0, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 27: Intent贸 'DOWN' -> Sali贸 'left' Cambio\n",
      "        Posici贸n: (0, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 28: Intent贸 'LEFT' -> Sali贸 'down' Cambio\n",
      "        Posici贸n: (1, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 29: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (1, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 30: Intent贸 'UP' -> Sali贸 'up' \n",
      "        Posici贸n: (0, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 31: Intent贸 'DOWN' -> Sali贸 'down' \n",
      "        Posici贸n: (1, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 32: Intent贸 'LEFT' -> Sali贸 'left' \n",
      "        Posici贸n: (1, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 33: Intent贸 'DOWN' -> Sali贸 'left' Cambio\n",
      "        Posici贸n: (1, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 34: Intent贸 'UP' -> Sali贸 'up' Choque\n",
      "        Posici贸n: (0, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 35: Intent贸 'DOWN' -> Sali贸 'down' \n",
      "        Posici贸n: (1, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 36: Intent贸 'LEFT' -> Sali贸 'left' \n",
      "        Posici贸n: (1, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 37: Intent贸 'UP' -> Sali贸 'right' Cambio\n",
      "        Posici贸n: (1, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 38: Intent贸 'LEFT' -> Sali贸 'left' \n",
      "        Posici贸n: (1, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 39: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (1, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 40: Intent贸 'LEFT' -> Sali贸 'left' \n",
      "        Posici贸n: (1, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 41: Intent贸 'LEFT' -> Sali贸 'stay' (Se qued贸 quieto)\n",
      "        Posici贸n: (1, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 42: Intent贸 'DOWN' -> Sali贸 'down' \n",
      "        Posici贸n: (2, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 43: Intent贸 'DOWN' -> Sali贸 'down' \n",
      "        Posici贸n: (3, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 44: Intent贸 'DOWN' -> Sali贸 'left' Cambio\n",
      "        Posici贸n: (3, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 45: Intent贸 'LEFT' -> Sali贸 'down' Cambio\n",
      "        Posici贸n: (4, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 46: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (4, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 47: Intent贸 'LEFT' -> Sali贸 'left' \n",
      "        Posici贸n: (4, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 48: Intent贸 'DOWN' -> Sali贸 'left' Cambio\n",
      "        Posici贸n: (4, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 49: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (4, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 50: Intent贸 'LEFT' -> Sali贸 'left' \n",
      "        Posici贸n: (4, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 51: Intent贸 'DOWN' -> Sali贸 'down' \n",
      "        Posici贸n: (5, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 52: Intent贸 'DOWN' -> Sali贸 'right' Cambio\n",
      "        Posici贸n: (5, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 53: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (5, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 54: Intent贸 'DOWN' -> Sali贸 'left' Cambio\n",
      "        Posici贸n: (5, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 55: Intent贸 'LEFT' -> Sali贸 'left' \n",
      "        Posici贸n: (5, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 56: Intent贸 'UP' -> Sali贸 'up' \n",
      "        Posici贸n: (4, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 57: Intent贸 'LEFT' -> Sali贸 'down' Cambio\n",
      "        Posici贸n: (5, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 58: Intent贸 'RIGHT' -> Sali贸 'down' Cambio\n",
      "        Posici贸n: (6, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 59: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (6, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 60: Intent贸 'DOWN' -> Sali贸 'down' \n",
      "        Posici贸n: (7, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 61: Intent贸 'LEFT' -> Sali贸 'down' Cambio\n",
      "        Posici贸n: (8, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 62: Intent贸 'DOWN' -> Sali贸 'down' \n",
      "        Posici贸n: (9, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 63: Intent贸 'DOWN' -> Sali贸 'left' Cambio\n",
      "        Posici贸n: (9, 0), Reward: 0\n",
      "------------------------------\n",
      "Paso 64: Intent贸 'UP' -> Sali贸 'right' Cambio\n",
      "        Posici贸n: (9, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 65: Intent贸 'RIGHT' -> Sali贸 'down' Cambio\n",
      "        Posici贸n: (9, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 66: Intent贸 'DOWN' -> Sali贸 'right' Cambio\n",
      "        Posici贸n: (9, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 67: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (9, 3), Reward: 0\n",
      "------------------------------\n",
      "Paso 68: Intent贸 'LEFT' -> Sali贸 'left' \n",
      "        Posici贸n: (9, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 69: Intent贸 'LEFT' -> Sali贸 'left' \n",
      "        Posici贸n: (9, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 70: Intent贸 'UP' -> Sali贸 'stay' (Se qued贸 quieto)\n",
      "        Posici贸n: (9, 1), Reward: 0\n",
      "------------------------------\n",
      "Paso 71: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (9, 2), Reward: 0\n",
      "------------------------------\n",
      "Paso 72: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (9, 3), Reward: 0\n",
      "------------------------------\n",
      "Paso 73: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (9, 4), Reward: 0\n",
      "------------------------------\n",
      "Paso 74: Intent贸 'DOWN' -> Sali贸 'down' \n",
      "        Posici贸n: (9, 4), Reward: 0\n",
      "------------------------------\n",
      "Paso 75: Intent贸 'UP' -> Sali贸 'right' Cambio\n",
      "        Posici贸n: (9, 5), Reward: 0\n",
      "------------------------------\n",
      "Paso 76: Intent贸 'LEFT' -> Sali贸 'up' Cambio\n",
      "        Posici贸n: (8, 5), Reward: 0\n",
      "------------------------------\n",
      "Paso 77: Intent贸 'RIGHT' -> Sali贸 'right' \n",
      "        Posici贸n: (8, 6), Reward: 0\n",
      "------------------------------\n",
      "Paso 78: Intent贸 'UP' -> Sali贸 'up' \n",
      "        Posici贸n: (7, 6), Reward: -1.0\n",
      "------------------------------\n",
      "\n",
      " LLego a una casilla terminal con Reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "board_10x10 = [[' ' for _ in range(10)] for _ in range(10)]\n",
    "board_10x10[0][0] = 'S'\n",
    "for c in [1, 2, 3, 4, 6, 7, 8]: board_10x10[2][c] = '#'\n",
    "for r in [3, 4, 5, 6, 7]: board_10x10[r][4] = '#'\n",
    "board_10x10[4][5] = '-1' \n",
    "board_10x10[5][5] = '+1' \n",
    "board_10x10[7][5] = '-1' \n",
    "board_10x10[7][6] = '-1'\n",
    "\n",
    "env3 = EnvironmentTask3(board_10x10)\n",
    "\n",
    "\n",
    "print(f\"Inicio en: {env3.current_state}\\n\")\n",
    "\n",
    "pasos = 0\n",
    "max_pasos = 1000\n",
    "\n",
    "while not env3.is_terminal() and pasos < max_pasos:\n",
    "    \n",
    "    intento = random.choice(['up', 'down', 'left', 'right'])\n",
    "    \n",
    "    recompensa, nuevo_estado, movimiento_real = env3.do_action(intento)\n",
    "    \n",
    "    info_extra = \"\"\n",
    "    if intento != movimiento_real and movimiento_real != 'stay':\n",
    "        info_extra = \"Cambio\"\n",
    "    elif movimiento_real == 'stay':\n",
    "        info_extra = \"(Se qued贸 quieto)\"\n",
    "    elif nuevo_estado == env3.initial_state and movimiento_real != 'stay':\n",
    "        info_extra = \"Choque\"\n",
    "        \n",
    "    print(f\"Paso {pasos}: Intent贸 '{intento.upper()}' -> Sali贸 '{movimiento_real}' {info_extra}\")\n",
    "    print(f\"        Posici贸n: {nuevo_estado}, Reward: {recompensa}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    pasos += 1\n",
    "\n",
    "if env3.is_terminal():\n",
    "    print(f\"\\n LLego a una casilla terminal con Reward: {recompensa}\")\n",
    "else:\n",
    "    print(f\"\\nSe detuvo por l铆mite de pasos ({max_pasos}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fb319",
   "metadata": {},
   "source": [
    "### Task 4.\n",
    "\n",
    "Defina una situaci贸n de la vide real (de su escogencia) como un MDP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c7342",
   "metadata": {},
   "source": [
    "Situacion : Un jugador de baloncesto practica lanzamientos de 3 puntos. El objetivo es maximizar la cantidad de puntos anotados en una sesi贸n. Se asume que el jugador tiene estados de \"confianza\" que afectan su precisi贸n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db780d",
   "metadata": {},
   "source": [
    "Estados (S):\n",
    "\n",
    "- Neutro : Estado inicial o despues de fallar\n",
    "- Racha : Despu茅s de encestar. Decimos que la confianza aumenta la precisi贸n.\n",
    "- Frio : Despu茅s de fallar 2 veces seguidas.\n",
    "- Fin : Termina la practica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13113c94",
   "metadata": {},
   "source": [
    "Acciones (A):\n",
    "\n",
    "- Tiro Est谩ndar: Un tiro con mec谩nica controlada y equilibrada.\n",
    "\n",
    "- Tiro R谩pido: Un tiro m谩s rapido. Permite sacar m谩s tiros en menos tiempo, pero con menor precisi贸n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e0d97",
   "metadata": {},
   "source": [
    "Probabilidades (P):\n",
    "\n",
    "| Tiro estandar | Estado | Neutro | Racha | Frio |\n",
    "| ------------- | ------ | ------ | ----- | ---- |\n",
    "| ----          | Neutro | 0,4    | 0,4   | 0,2  |\n",
    "| ----          | Racha  | 0,4    | 0,6   | 0    |\n",
    "| ----          | Frio   | 0,3    | 0     | 0,7  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f5944",
   "metadata": {},
   "source": [
    "| Tiro rapido | Estado | Neutro | Racha | Frio |\n",
    "| ----------- | ------ | ------ | ----- | ---- |\n",
    "| ----        | Neutro | 0,3    | 0,3   | 0,4  |\n",
    "| ----        | Racha  | 0,5    | 0,5   | 0    |\n",
    "| ----        | Frio   | 0,2    | 0     | 0,8  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0d376",
   "metadata": {},
   "source": [
    "Recompensas (R):\n",
    "\n",
    "- +3 por cada enceste (茅xito).\n",
    "\n",
    "- -1 por cada fallo.\n",
    "\n",
    "- +1 extra si se encesta usando Tiro R谩pido (por la satisfacci贸n de meter un tiro dif铆cil), siendo entonces un total de +4.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
